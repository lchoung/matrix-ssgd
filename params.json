{
  "name": "GPU Based SSGD Matrix Factorization",
  "tagline": "GPU Based Large Scale Matrix Factorization with Stochastic Gradient Descent",
  "body": "### Proposal\r\n\r\nGPU Based Large Scale Matrix Factorization with Stochastic Gradient Descent\r\n\r\nLillian Choung (lchoung)\r\n\r\n### Summary\r\n\r\nI plan to implement a parallel and distributed version of Non-Negative Matrix Factorization using the Stratified Stochastic Gradient Descent algorithm configuration with CUDA.\r\n\r\n### Background\r\n\r\nMatrix factorization has applications in document clustering, recommendations, and signal processing. The core idea involves breaking a matrix M down to latent factor matrices A and B such that AB approximates M as much as possible according to some loss function L (potentially with some regularization to avoid overfitting). \r\n\r\nTo find A and B, I will use the Stochastic Gradient Descent method. The following repeats until convergence:\r\n\r\n`For each matrix square in M:`\r\n\r\n`   --> Calculate the loss of the square in AB, compared to M.`\r\n\r\n`   --> Adjust the elements of A and B that contribute towards the square, opposite the direction of the gradient.`\r\n\r\nAn alternative to pure SGD is Stratified Stochastic Gradient Descent, as proposed by [Gemulla et. al.](http://people.mpi-inf.mpg.de/~rgemulla/publications/gemulla11dsgd.pdf). We can block a matrix and perform operations on each block concurrently, by selecting blocks whose necessary adjustments in A and B will not coincide with each other. We will use CUDA to parallelize these operations on blocks. \r\n\r\n### Challenge\r\n\r\nThe one main challenge I foresee in this project:\r\n\r\n1. Since matrices large enough to warrant GPU speedup may prove to be billions of rows long, they may not fit in memory of a single computer. I may have to investigate a method of speaking between several GPUs (i.e. MPI). \r\n\r\n2. Following (1): I must synchronize the values of A and B at the end of each iteration if I move towards a distributed CUDA system. The benefit of staying on GPU is that it lacks the latency of copying memory back and forth. Therefore, I will need to be careful about communication overhead. \r\n\r\n### Resources\r\n\r\n[SSGD: Gemulla et. al.](http://people.mpi-inf.mpg.de/~rgemulla/publications/gemulla11dsgd.pdf)\r\n[Article on SSGD](http://www.almaden.ibm.com/cs/people/peterh/dsgdTechRep.pdf)\r\n\r\n[OpenMPI + CUDA example](http://www.cse.buffalo.edu/faculty/miller/Courses/CSE710/heavner.pdf)\r\n\r\nI plan to run experiments upon the latedays cluster. \r\n\r\n### Goals\r\n\r\n1. Implement parallelized SSGD on CUDA for one machine\r\n\r\n2. Distribute on multiple machines with MPI\r\n\r\n3. Compare performance on [Netflix set](http://academictorrents.com/details/9b13183dc4d60676b773c9e2cd6de5e5542cee9a) with in-memory R and Hadoop implementations mentioned in Gemulla. (If time)\r\n\r\n### Timeline\r\n\r\n    Week 1: Achieve familiarity with tools / Implement SSGD on CUDA (one machine)\r\n    Week 2: Implement SSGD on CUDA\r\n    Week 3: Distributed implementations with MPI\r\n    Week 4: Distributed implementation with MPI\r\n    Week 5: Tuning parameters/Testing + Final paper\r\n\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}